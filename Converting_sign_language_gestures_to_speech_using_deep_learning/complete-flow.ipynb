{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "eb9d5808",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import json\n",
    "import numpy as np\n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import time\n",
    "import os\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from gtts import gTTS\n",
    "from fractions import Fraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7be7b90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset/WLASL/info.json' , 'r') as ds_info_f:\n",
    "    ds_info = json.load(ds_info_f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ca47f6dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2000"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4158bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique glosses, gloss to video_id map, signer info with gloss + video_id\n",
    "\n",
    "def get_required_info(ds_info, remove_missing=True):\n",
    "    glosses = []\n",
    "    gloss_video_id_map = {}\n",
    "    signer_info = {}\n",
    "\n",
    "    for word in ds_info:\n",
    "        gloss = word['gloss']\n",
    "        glosses.append(gloss)\n",
    "\n",
    "        for inst in word['instances']:\n",
    "            if (not os.path.exists(f'dataset/WLASL/videos/{inst[\"video_id\"]}.mp4')) and remove_missing:\n",
    "                continue\n",
    "            \n",
    "            if gloss not in gloss_video_id_map:\n",
    "                gloss_video_id_map[gloss] = []\n",
    "\n",
    "            gloss_video_id_map[gloss].append(inst['video_id'])\n",
    "\n",
    "            s_id = inst['signer_id']\n",
    "\n",
    "            if s_id not in signer_info:\n",
    "                signer_info[s_id] = {}\n",
    "\n",
    "            if gloss not in signer_info[s_id]:\n",
    "                signer_info[s_id][gloss] = []\n",
    "                \n",
    "            signer_info[s_id][gloss].append(inst['video_id'])\n",
    "        \n",
    "        gloss_video_id_map[gloss].sort()\n",
    "            \n",
    "    return glosses, gloss_video_id_map, signer_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c62f45e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "glosses, gloss_video_id_map, signer_info = get_required_info(ds_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ac37da19",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['better', 'late', 'than', 'never']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "09943a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [i for i in range(len(words))]\n",
    "y = to_categorical(y).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8a0a433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'better': array([1, 0, 0, 0]),\n",
       " 'late': array([0, 1, 0, 0]),\n",
       " 'than': array([0, 0, 1, 0]),\n",
       " 'never': array([0, 0, 0, 1])}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_map = {label:y[num] for num, label in enumerate(words)}\n",
    "label_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2beabb58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract frames from the video file\n",
    "# video: path of the video file\n",
    "# n_frames: required number of frames to extract from the file\n",
    "\n",
    "from fractions import Fraction\n",
    "\n",
    "def save_holistic(path, holistic, image):\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    image.flags.writeable = False\n",
    "    results = holistic.process(image)\n",
    "    image.flags.writeable = True\n",
    "\n",
    "    pose = np.array([[res.x, res.y, res.z, res.visibility] for res in results.pose_landmarks.landmark]).flatten() if results.pose_landmarks else np.zeros(33*4)\n",
    "    face = np.array([[res.x, res.y, res.z] for res in results.face_landmarks.landmark]).flatten() if results.face_landmarks else np.zeros(468*3)\n",
    "    lh = np.array([[res.x, res.y, res.z] for res in results.left_hand_landmarks.landmark]).flatten() if results.left_hand_landmarks else np.zeros(21*3)\n",
    "    rh = np.array([[res.x, res.y, res.z] for res in results.right_hand_landmarks.landmark]).flatten() if results.right_hand_landmarks else np.zeros(21*3)\n",
    "\n",
    "    landmarks = np.concatenate([pose, face, lh, rh])\n",
    "    np.save(path, landmarks)\n",
    "    \n",
    "def sequential_frames(v_id, src, holistic, n_frames):\n",
    "    for f in range(n_frames):\n",
    "        ret, frame = src.read()\n",
    "        if ret:\n",
    "            save_holistic(f'./wlasl-info/landmarks-sentence/{v_id}/{f}.npy', holistic, frame)\n",
    "        else:\n",
    "            np.save(f'./wlasl-info/landmarks-sentence/{v_id}/{f}.npy', np.zeros((1662, )))\n",
    "\n",
    "def frames_from_video(v_id, n_frames):\n",
    "    mp_drawing = mp.solutions.drawing_utils\n",
    "    mp_drawing_styles = mp.solutions.drawing_styles\n",
    "    mp_holistic = mp.solutions.holistic\n",
    "    \n",
    "    with mp_holistic.Holistic(min_detection_confidence=0.5, min_tracking_confidence=0.5) as holistic:\n",
    "        src = cv2.VideoCapture(f'./dataset/WLASL/videos/{v_id}.mp4')\n",
    "        video_length = src.get(cv2.CAP_PROP_FRAME_COUNT)\n",
    "        os.makedirs(f'./wlasl-info/landmarks-sentence/{v_id}', mode=777)\n",
    "        \n",
    "        if video_length <= n_frames:\n",
    "            frame_step = 0\n",
    "            sequential_frames(v_id, src, holistic, n_frames)\n",
    "        else:\n",
    "            f = 0\n",
    "            ratio = round(video_length/n_frames, 1)\n",
    "            frac = Fraction(ratio).limit_denominator(10)\n",
    "            \n",
    "            if ratio == 1.7 or ratio == 1.9:\n",
    "                frac = Fraction(ratio).limit_denominator(2)\n",
    "            \n",
    "            num, den = frac.numerator, frac.denominator\n",
    "            while f < n_frames:\n",
    "                for _ in range(min(den, n_frames-f)):\n",
    "                    ret, frame = src.read()\n",
    "                    if ret:\n",
    "                        save_holistic(f'./wlasl-info/landmarks-sentence/{v_id}/{f}.npy', holistic, frame)\n",
    "                    else:\n",
    "                        np.save(f'./wlasl-info/landmarks-sentence/{v_id}/{f}.npy', np.zeros((1662, )))\n",
    "                    f += 1\n",
    "                for _ in range(num-den):\n",
    "                    ret, frame = src.read()\n",
    "\n",
    "        src.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30b8b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gloss in words:\n",
    "#     for v_id in gloss_video_id_map[gloss]:\n",
    "#         frames_from_video(v_id, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "7c15644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_val_test_data(glosses, label_map, split_ratio=[0.8, 0.9]):\n",
    "    \n",
    "    result_train_v_ids = []\n",
    "    result_val_v_ids = []\n",
    "    result_test_v_ids = []\n",
    "    result_train_labels = []\n",
    "    result_val_labels = []\n",
    "    result_test_labels = []\n",
    "    \n",
    "    for gloss in glosses:\n",
    "        v_ids = gloss_video_id_map[gloss]\n",
    "        split_1 = int(len(v_ids) * (split_ratio[0]))\n",
    "        split_2 = int(len(v_ids) * (split_ratio[1]))\n",
    "        result_train_v_ids.extend(v_ids[:split_1])\n",
    "        result_val_v_ids.extend(v_ids[split_1:split_2])\n",
    "        result_test_v_ids.extend(v_ids[split_2:])\n",
    "        result_train_labels.extend([label_map[gloss]]*split_1)\n",
    "        result_val_labels.extend([label_map[gloss]]*(split_2-split_1))\n",
    "        result_test_labels.extend([label_map[gloss]]*(len(v_ids) - split_2))\n",
    "        \n",
    "    return np.array(result_train_v_ids), np.array(result_train_labels), np.array(result_val_v_ids), np.array(result_val_labels), np.array(result_test_v_ids), np.array(result_test_labels) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "abd856a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a, b, c, d, e, f_ = get_train_val_test_data(words, label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1d008589",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((37,), (37, 4), (5,), (5, 4), (7,), (7, 4))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape, b.shape, c.shape, d.shape, e.shape, f_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1714351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(tf.keras.utils.Sequence):\n",
    "    def __init__(self, v_ids, labels, n_frames, batch_size = 2, training = False):\n",
    "        self.v_ids = v_ids\n",
    "        self.labels = labels\n",
    "        self.n_frames = n_frames\n",
    "        self.training = training\n",
    "        self.batch_size = batch_size\n",
    "        self.indexes = np.arange(len(self.v_ids))\n",
    "        if self.training:\n",
    "            np.random.shuffle(self.indexes)\n",
    "\n",
    "    def get_video_frames(self, v_ids, classes):\n",
    "        result_X = []\n",
    "        result_y = []\n",
    "        for index, v_id in enumerate(v_ids):\n",
    "            window = []\n",
    "            for f in range(self.n_frames):\n",
    "                window.append(np.load(f'./wlasl-info/landmarks-sentence/{v_id}/{f}.npy'))\n",
    "            result_X.append(np.stack(window, axis=0))\n",
    "            result_y.append(classes[index])\n",
    "        \n",
    "        return np.array(result_X), np.array(result_y)\n",
    "            \n",
    "    def __getitem__(self, index):\n",
    "        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "        return self.get_video_frames(self.v_ids[indexes], self.labels[indexes])\n",
    "    \n",
    "    def __len__(self):\n",
    "        return math.floor(len(self.v_ids) / self.batch_size)\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        if self.training:\n",
    "            np.random.shuffle(self.indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92546c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = DataGenerator(a, b, 30, training=True)\n",
    "val_gen = DataGenerator(c, d, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "22a924eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer gru_9 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_10 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer gru_11 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    }
   ],
   "source": [
    "model = keras.models.load_model('models/wlasl-sentence')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4292145d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_speech(folder, model, words):\n",
    "    total_dir = 0\n",
    "    for base, dirs, files in os.walk(folder):\n",
    "        for directories in dirs:\n",
    "            total_dir += 1\n",
    "    v_ids = range(1, total_dir+1)\n",
    "    sentence = []\n",
    "    for index, v_id in enumerate(v_ids):\n",
    "        window = []\n",
    "        for f in range(30):\n",
    "            window.append(np.load(f'./wlasl-info/landmarks-sentence-test/{v_id}/{f}.npy'))\n",
    "        sentence.append(words[np.argmax(model.predict(np.array([window])))])\n",
    "    \n",
    "    sentence = ' '.join(sentence)\n",
    "    speech = gTTS(text=sentence, lang='en', slow=False)\n",
    "    speech.save(\"output.mp3\")\n",
    "    os.system(\"start output.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "427c85ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 2s 2s/step\n",
      "1/1 [==============================] - 0s 231ms/step\n",
      "1/1 [==============================] - 0s 187ms/step\n",
      "1/1 [==============================] - 0s 172ms/step\n"
     ]
    }
   ],
   "source": [
    "generate_speech('./wlasl-info/landmarks-sentence-test/', model, words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d93f3e0a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
